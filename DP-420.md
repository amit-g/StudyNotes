* Account
  * Unit of distribution and high availability
  * Defines the DNS name

* Database
  * Unit of management
  * User, permissions are configured at this level
  * Throughput can also be provisioned

* Container
  * Unit of scalability
  * Throughput is typically configured at this level

* Gateway
  * Client SDK must use
    * Gateway connection string which is {accountName}.sqlx.cosmos.azure.com vs {accountName}.documents.azure.com
    * Use GatewayMode (instead of Direct mode)
    * Use Session or Eventual consistent

* Consistency
  * Levels
    * Strong
      * Not supported in multi-region & multi-write deployment
    * Bounded Staleness
      * Maximum staleness can be configured in two ways
        * The number of versions (K) of the item
        * The time interval (T) reads might lag behind the writes
    * Session
      * Default
      * Reads are guaranteed to honor the read-your-writes, and write-follows-reads guarantees
    * Consistent Prefix
      * Updates made as a batch within a transaction, are returned consistent to the transaction in which they were committed
      * Reads can lag behind but will always be in the same order as written
    * Eventual
      * Reads can be out of order even within the same region (if different replica is read)
  * Consistency level can be overridden (relaxed only) for individual requests
  * Overriding the default consistency level only applies to reads within the SDK client
  * Consistency levels and latency
    * The read latency for all consistency levels is guaranteed to be less than 10ms at 99th percentile. Avg is 4ms at 50th percentile
    * The write latency for all consistency levels is always guaranteed to be less than 10ms at 99th percentile. Avg is 5ms at 50th percentile. Multi-region accounts with strong consistency are an exception
  * For strong and bounded staleness the reads are done from 2 of 4 replicas in a region thus reducing the throughput by half when compared to other consistency levels where only one replica is read
  * With exception of strong consistency, write in all others follow local majority i.e. 3 out of 4 replicas. In strong consistency global majority is needed for committing any transaction
  * RPO varies from 0 for strong consistency for a multi region account to 240 for single region account
  * Bounded staleness always has K & T as RPO which
    * Minimum value of K and T is 10 write operations or 5 seconds for single region account
    * Minimum value of K and T is 100,000 write operations or 300 seconds for multi region account

* NOSQL Queries
  * Point read - Only via SDK, must specify partition key
    * 1KB document accessed will cost 1 RU
  * Minimum RU consumption for any query is 2.3
  * Use `SELECT VALUE` to flatten
  * Opreators
    * Usual and || for concatenation
  * ORDER BY
    * Requires that there is an index on the field being sorted
    * Requires composite index if multiple properties are used for sorting
    * Documents with missing fields are returned
  * OFFSET LIMIT
    * ORDER BY is not mandatory
    * Server still incurrs cost to fetch the items using this method
    * Use of ContinuationToken is preferable over this for paging

* Indexing
  * Modes
    * Consistent - index is updated synchronously with CUD operations
    * None - no indexing
    * Lazy - cannot be enabled without contacting support, not available with serverless option
  * Retrievable, Filterable, Sortable, Searchable
  * Any indexing policy has to include the root path /* as either an included or an excluded path
  * In consistent indexing mode, the system properties id and _ts are automatically indexed
  * The system property _etag is excluded from indexing by default
  * /propertyPath/* - any property path starting with /propertyPath
  * /propertyPath/? - only property path with /propertyPath and ending in scalar value
  * propertyPath/[] - any property path starting with /propertyPath ending in array
  * Equality properties should always be included first in the filter to get better performance
  * Query Metrics
    * Retrieved Document Count vs Output Document Count
  * Containers with TTL activated can't have indexing mode set to None and vice versa
  * Index usage by system functions
    * Most system functions e.g. StartsWith, Contains, RegexMatch, Left, Substring (- but )only if the first num_expr is 0) use index
    * Some system functions e.g. Upper/Lower, GetCurrentDateTime/GetCurrentTimestamp/GetCurrentTicks, Mathematical functions do not use index
    * Spatial system functions us index except when used in queries with agggregations
  * In many cases queries could be even more optimized by adding order by clause

* Composite indexes
  * Queries that have an order by clause with more than one properties require a composite index (with all properties)
  * Every path in composite index has an implicit /? at the end
  * The order of the properties as well as sort order in the composite index must match exactly as given in the query
  * The filter clause that have same properties can cause the query to utilize the composite index. In this case, the ASC/DESC doesn't matter
    * The properties with equality property must be first one in order to filter to be utilized
  * The filter clause with multiple range filters need multiple composite indexes, each with its own properties
  * Composite indexes can support at most one range filter
  * [Queries with filters on multiple properties](https://learn.microsoft.com/en-us/azure/cosmos-db/index-policy#queries-with-filters-on-multiple-properties)
  * [Queries with a filter and ORDER BY](https://learn.microsoft.com/en-us/azure/cosmos-db/index-policy#queries-with-a-filter-and-order-by)
  * [Queries with a filter and an aggregate](https://learn.microsoft.com/en-us/azure/cosmos-db/index-policy#queries-with-a-filter-and-an-aggregate)
  * [New ways to use composite indexes in Azure Cosmos DB](https://devblogs.microsoft.com/cosmosdb/new-ways-to-use-composite-indexes/)

* Indexing References
  * [Troubleshoot Query Performance](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/troubleshoot-query-performance)
  * [Query Performance Indexing Metrics](https://devblogs.microsoft.com/cosmosdb/query-performance-indexing-metrics/)
  * [Index Policy](https://learn.microsoft.com/en-us/azure/cosmos-db/index-policy)
  
* Unique Keys
  * Uniqueness is guaranteed within a logical partition only
  * It results in higher RUs for CRUD operations
  * Sparse unique keys are not supported i.e. there could be a single null or missing key value for the key per logical partition
  * Unique keys are case sensitive
  * A unique key policy can have a maximum of 16 path values
  * Each unique key policy can have at most 10 unique key constraints or combinations

* Physical Partitions
  * Automatically managed by CosmosDB
  * Max RUs - 10000
  * Max data - 50GB
    * Manual throughput provisioned accounts will automatically split the partition when the 60% of 50GB or 10000RUs is used up
    * Autoscale throughput provisioned accounts will automatically split the partition when the 100% of 50GB or 10000RUs is used up

* Logical Partitions
  * Determined by partition key
  * Max data - 20GB

* SLA
  * Single region without availability zones - 99.99
  * Single region with availability zones - 99.995
  * Multiple region, multi write (with or without availability zones) - 99.999

* Connectivity
  * Direct Mode is more performant as it connects to the backend replica on a TCP ports (10000-20000)
  * Gateway mode is less performant but is firewall friendly as it uses 443 that is typically open (0 to 65535)
    * SQL (443), MongoDB (10250, 10255, 10256), Table (443), Cassandra (10350), Graph (443) 
  * Direct mode connection to Private link accounts require full range of TCP ports (0-65535) to be open
  * Gateway mode is recommended for low connection limit
  * Gateway+HTTPS is the default transport mode in .NET SDK v2
  * Direct+TCP is the default transport mode in .NET SDK v3

* Dedicated Gateway with integrated cache
  * Only supported for SQL API
  * Not supported with private link
  * Can't be used for accounts in VNET
  * Can't be used with accounts deployed in multiple availability zones
  * Doesn't support RBAC for data plane requests

* .NET SDK
  * Gateway+HTTPS is the default transport mode in .NET SDK v2
  * Direct+TCP is the default transport mode in .NET SDK v3
  * ItemID is not automatically generated in .NET SDK v3
  * CosmosClientOptions for client level options e.g. ConnectionMode
  * [QueryRequestOptions](https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.queryrequestoptions?view=azure-dotnet) for query level options e.g. ConsistencyLevel, PopulateIndexMetrics, MaxItemCount, MaxBufferItemCount 
  * ItemRequestOptions for individual request options e.g. ConsistencyLevel, DedicatedGatewayRequestOptions
  * DedicatedGatewayRequestOptions  for gateway options e.g. MaxIntegratedCacheStaleness 
  * FeedResponse<T> contains RequestCharge, IndexMetrics, Diagnostics, Count, StatusCode
  * CosmosClientOptions.PortReuseMode should be set to PrivatePortPool for spiky workloads
  * CosmosClientOptions.IdleConnectionTimeout set to more than 10 minutes would allow mitigation of ephemeral port exhaustion
  * CosmosClientOptions.MaxRetryWaitTimeInSeconds default is 30
  * CosmosClientOptions.MaxRetryAttemptsOnThrottledRequests default is 9
  * SDK allows point read query which costs 1 RU for 1KB document. Same query via SQL will cost about 2.3 RUs
    * `var order = await container.ReadItemAsync<Order>(id: "order0103", partitionKey: new PartitionKey("TimS1234"));`
    * `SELECT * FROM c WHERE c.id = "order0103" AND c.customerId = "TimS1234"`
  * ResponseMessage.RequestCharge - Consumed RUs
  * ResponseMessage.Diagnostics - Can also be retrieved from ItemResponse, FeedResponse and CosmosException
  * Bulk Batch Execution
    * Triggered by AllowBulkExecution flag on CosmosClientOptions
    * Concurrent operations are grouped together by physical partition affinity
    * Bulk batch have limit of 100 parallel operations
    * The size also have a limit of 2MB of documents per batch
    * The batch continues to wait for 100 milliseconds and if the batch is not full, it dispatches the batch
    * This mode allows using more throughput so volume of data is higher and thus the consumed RUs are higher
    * To get better performance, provide the partition key and use stream API if the incoming data is already streamed to avoid serialization/deserialization
    * Rate Limit Controls
      * `CosmosClientOptions.MaxRetryAttemptsOnRateLimitedRequests`
      * `CosmosClientOptions.MaxRetryWaitTimeOnRateLimitedRequests`
  * Bulk Executor Library
    * Reduces client side compute resources
    * Moved to SDK v3 (no longer a standalone library)
      * Set `AllowBulkExecution = true`
    * Supported for SQL and Gremlin API
    * Default options should be overridden to allow Bulk Executor control these parameters (default is 30 seconds and 9 retries)
      * `client.ConnectionPolicy.RetryOptions.MaxRetryWaitTimeInSeconds = 0;`
      * `client.ConnectionPolicy.RetryOptions.MaxRetryAttemptsOnThrottledRequests = 0;`
  * SDK Bulk Operations
    * [bulk-support-in-the-net-sdk](https://devblogs.microsoft.com/cosmosdb/introducing-bulk-support-in-the-net-sdk/)
    * [tutorial-dotnet-bulk-import](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/tutorial-dotnet-bulk-import)

* Java SDK
  * idleEndpointTimeout - default 0. Minimum 60 minutes. Recommended to set 2-3 hours for sporadic workload

* Spatial Data
  * Supports GeoJSON points, polygons, multi-polygons and LineStrings
  * GeoSpatial data `
    "location":{
        "type":"Point",
        "coordinates":[ -122.12, 47.66 ]
    }
    `
  * In case of geography, the coordinates are latitude and longitude
  * `SpatialPath { Path = /location/* }` would mean CosmosDB would index all valid points, polygons, multi-polygons and LineStrings
  * If any of the type is indexed all other types are indexed too
  * One Geospatial Configuration per container: geography or geometry
    * Default if the geography
  * BoundingBox can only be enabled for Geography or Geometry data for a container
  * Geography Indexing is configured using
    
    `
      "spatialIndexes": [
        {
            "path": "/*",
            "types": [
                "Point",
                "Polygon",
                "MultiPolygon",
                "LineString"
            ]
        }
      ]
    `
  * Geometry Indexing is configured using
  
    `
      "spatialIndexes": [
        {
            "path": "/locations/*",
            "types": [
                "Point",
                "LineString",
                "Polygon",
                "MultiPolygon"
            ],
            "boundingBox": {
                "xmin": -10,
                "ymin": -20,
                "xmax": 10,
                "ymax": 20
            }
        }
      ]
    `
    * Bounding box is mandatory in Geometry
      * Only operations computed on the objects that are entirely inside the bounding box will be able to utilize the spatial index
      * Larger bounding box would result in lower performance
    * Earth is the bounding box in case of Geography
    * Query allows operators like ST_DISTANCE, ST_WITHIN, ST_INTERSECTS, ST_ISVALID and ST_ISVALIDDETAILED 

* Conflict Resolution
  * Default policy is Last Write Wins (LWW) using _ts property
    * These conflicts do not show up in conflict feed
    * A different property can be specified in place of _ts (only for SQL API)
  * Custom policy can be implemented as a stored procedure
    * incomingItem, existingItem, isTombstone, conflictingItems
  * In case of partial update operations (patch), LWW or path based conflict resolution policy with trigger. If the same property is being updated by two clients, LWW will be used but if different properties are being updated by two clients the path based conflict resolution policy will be used

* Partial Document Updates
  * Operation types Add, Set, Replace, Remove, Increment are supported
  * Single document mode can have 10 operations
  * Multiple documents can be patched as long as the partition key is same
  * Conditional updates - filter clause can be applied
  * Add vs Set
    * Identical except for Array where Add adds the element at a given index shifting other elements while Set updates the existing element at the given index
  * Add vs Replace
    * Add operation adds a property if it doesn't already exist (including Array data type). Replace operation fails if the property doesn't exist (applies to Array data type as well)
  * Set vs Replace
    * Set operation adds a property if it doesn't already exist (except if there was an Array). Replace operation fails if the property doesn't exist (applies to Array data type as well)

* Serverless
  * Can only run in a single region
  * Allows up to 50GB per container
  * Can offer maximum of 5,000 RU/s

* Provisioned Throughput - Autoscale
  * Cost effective if Max throughput is used less than 66% of hours per month
  * 0.1 * TMax < T < TMax where TMax is highest throughput

* Provisioned Throughput - Standard
  * Cost effective if Max throughput is used more than 66% of hours per month
  * Requests will be rate limited if required RUs exceed provisioned throughput
  * Single database can have up to 25 containers sharing the RUs

* Sizing
  * Provisioned RU/s = C * T / R
    * T = Total vCores and/or vCPUs in current environment
    * R = Replication factor
    * C = 600 RU/s/vCore for SQL API
    * C = 1000 RU/s/vCore for Mongo API
    * Not available for other APIs
  * If the hyper-threading isn't supported, 1 vCore can be used for 1 vCPU
  * If replication factor is unknown, it is recommended to use 3
  * Minimum throughput on container with manual throughput is maximum of
    * 400 RU/s
    * Current storage in GB * 1 RU/s
    * Higest RU/s ever provisioned / 100
  * Minimum throughput on shared throughput database is maximum of 
    * 400 RU/s
    * Current storage in GB * 1 RU/s
    * Higest RU/s ever provisioned / 100
    * 400 + MAX(ContainerCount - 25, 0) * 100 RU/s

* TTL (transactional store)
  * Max is 2147483647 seconds, ~24,855 days or ~68 years
  * Leftover RUs are used by background task
  * Value of -1 means that there is no default TTL at container level and item level TTL will be used (if available)

* CLI
  * Default throughput 400 RUs (if not provided in command line)
  * --analytical-storage-schema-type {FullFidelity, WellDefined}
  * --default-consistency-level {BoundedStaleness, ConsistentPrefix, Eventual, Session, Strong}
  * --enable-free-tier {false, true}
  * --enable-public-network {false, true}
  * --ip-range-filer for allowed list of IPs in the firewall
  * --network-acl-bypass {AzureServices, None}
  * --locations regionName=eastus failoverPriority=0 isZoneRedundant=False --locations
  * Specify region failover locations and priorities
    * `az cosmosdb update --name $account --resource-group $resourceGroup --locations regionName="$location" failoverPriority=0 isZoneRedundant=False --locations regionName="$failoverLocation1" failoverPriority=1 isZoneRedundant=False --locations regionName="$failoverLocation2" failoverPriority=2 isZoneRedundant=False`
  * Change the failover priority
    * `az cosmosdb failover-priority-change --name $account --resource-group $resourceGroup --failover-policies "$location=0" "$failoverLocation1=2" "$failoverLocation2=1"`

* Status Codes
  * Create
    * 201 Created
    * 400 Bad Request - Json document is invalid
    * 403 Unauthorized
    * 409 Conflict - id for the new document is already taken by an existing document
    * 413 Entity Too Large
  * Get a document
    * 404 Not Found
    * 400 Bad Request - x-ms-consistency-level header is stronger than account level consistency
    * 302 Not Modified - the document is not modified since the specified eTag value in the If-Match header
  * Replace a document
    * 404 Not Found
    * 400 Bad Request - Json document is invalid
    * 302 Not Modified - the document is not modified since the specified eTag value in the If-Match header
    * 409 Conflict - id for the new document is already taken by an existing document
    * 413 Entity Too Large
  * Patch
    * 400 Bad Request - Json document is invalid
    * 412 Precondition Failed - Using If-Match (Etag) header in case of OCC (Optimistic Concurrency Control)
  * Delete a document
    * 204 No Content
    * 404 Not Found
  * Query
    * 400 Bad Request - Invalid SQL
  * General
    * 408 Request Timeout - Usually an SP or query not completed within a given timeframe
    * 429 Too Many Requests
    * 500 Internal Server Error
    * 503 Service Unavailable
    * 423 Another scaling operation is in progress

* Data Migration
  * While using ADF sink configuration writeBatchSize should be set to appropriate value to avoid Request Size Exceeded error. Single request size can't exceed 2 MB so writeBatchSize should be set to 2MB / Single Document Size
  * Using BulkExecutorLibrary, the wait parameters should be set to 0 so as to allow the library to control these values

* Backups
  * Periodic backups are automatic and default
    * Full every 4 hours, last two backups retained
      * Backup interval - 1 to 24 hours - Default 4 hours
      * Backup Retention - 2 times the backup interval to 30 days at most - Default 8 hours
      * Backup storage redundancy - GRS, ZRS or LRS - Default GRS
    * Deleted databases and containers snapshots kept for 30 days
    * Backed up data is stored in the write region (or one of the write region if multi-region write is enabled)
    * Restore can happen only by a support ticket
      * Account is restored to <OriginalAccountName>-restored#
      * Firewall, VNET, private endpoint, consistency settings, stored procedures, triggers, UDFs, regional configs etc are not restored
      * Azure Support Standard, Developer or higher plan is needed. Basic plan doesn't allow support tickets  
      * In case of data corruption in a container, the data retention should be increased so as not to accidentally override the data (default is only 8 hours for 2 backups)
    * First two backups are free, any extra cost additional
    * No impact on performance consume any RUs
  * Continuous backups
    * Retained for 30 days (can't be changed)
      * Now there are 2 tiers - 7 days and 30 days
      * 7 day tier doesn't charge for storage while 30 days has additional charge
      * Restores are charged for both tiers
    * Local to region or zone redundant if the account is zone redundant
    * Multi-region write accounts can't use this
    * Only accounts with SQL and Mongo are supported. Accounts with Table API and Gremlin are in preview. Accounts with Cassandra API are not supported
    * Accounts with Synapse link can't use this
    * Typically mutations are backed up within 100 seconds
    * Restores (point-in-time) are at account level and can be done in portal, CLI or Powershell
  * Analytic store data isn't backed up
 
* Monitoring
  * Metrics are collected every minute and retained for 30 days by default
  * Cosmos DB Portal displays throughput, storage availability, latency, consistency, and system level metrics. The retention for these are 7 days
  * Resource logs are only collected if diagnostic settings are configured
    * AzureDiagnostics and Azure Metrics are collected by default.
    * Filter by ResourceProvider == "MICROSOFT.DOCUMENTDB"
    * Throughput Update
      * `
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.DOCUMENTDB" and  Category = 'ControlPlaneRequests'
| where OperationName = 'SqlContainersThroughputUpdated'`
    * Slow Queries
      *  `
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.DOCUMENTDB" and Category == "DataPlaneRequests"
| project TimeGenerated, duration_s
| summarize count() by bin(TimeGenerated, 5s)
| render timechart`

  * A typical application should have less than 5% of total errors as rate limiting errors (429 - Too Many Requests)
  * Diagnostic logs
    * DataPlaneRequests - (CDBDataPlaneRequests) This table logs back-end requests for operations that execute create, update, delete, or retrieve data. It captures values for RequestCharge, StatusCode, ClientIpAddress, PartitionId, RequestTokenPermissionId, RequestTokenPermissionMode
    * QueryRuntimeStatistics - This table logs query operations against the NoSQL API account. It includes PartitionKeyRangeId
    * PartitionKeyStatistics - This table logs logical partition key statistics in estimated KB. It's helpful when troubleshooting skew storage. This includes estimated storage per logical partition. This table is only populated if at least 1% of items are in single logical partition and only top 3 keys with largest partitions are returned
      * `
CDBPartitionKeyStatistics
| summarize arg_max(TimeGenerated, *) by AccountName, DatabaseName, CollectionName, _ResourceId, PartitionKey
| extend utilizationOf20GBLogicalPartition = SizeKb / (20.0 * 1024.0 * 1024.0)
| project TimeGenerated, AccountName, DatabaseName, CollectionName, _ResourceId, PartitionKey, SizeKb, utilizationOf20GBLogicalPartition`
    * PartitionKeyRUConsumption - This table logs every second aggregated RU/s consumption of partition keys. It's helpful when troubleshooting hot partitions. This includes RequestCharge
    * ControlPlaneRequests - This table logs Azure Cosmos DB account control data, for example adding or removing regions in the replication settings
 
* Troubleshooting
  * 429 Too Many Requests
    * Normalized RU Consumption (%) By PartitionKeyRangeID in Throughput tab
    * Up to 5% is good utilization of allocated RUs
    * To identifying the root cause, run a Kustos query on table AzureDiagnostics with  Category == "DataPlaneRequests"
    * A high volume of metadata operations (list, create, modify, or delete database or containers) can also cause 429 exceptions

* Security
  * IP Access Control
    * Single IP, CIDR or VNET
    * Allow access from Azure portal
  * Data encryption
    * Storage uses SSDs
    * Backups uses HDDs
    * Encryption at rest (AES-256) is auto configured using Microsoft managed keys
      * Allows customer managed keys
        * Locations must be specified when using self-managed keys
        * Locations properties must have regionNames, failoverPriority and isZoneRedundant is mandatory too
  * RBAC
    * DocumentDB Account Contributor
    * CosmosDB Account Reader - Can open support tickets
    * CosmosBackupOperator - Can open support tickets to request backup and restore operations for periodic backup. No data access is allowed
    * Cosmos Restore Operator - Can perform a restore action for an Azure Cosmos DB account with continuous backup mode. Typically used along with Cosmos DB Operator
    * Cosmos DB Operator - Can provision account, DB, containers but not access data, Can open support tickets
    * Can lock down changes from SDK using account keys i.e. the data read/write are the only actions that are allowed
      * Creation of databases, containers, reading/updating throughput, modifying container properties e.g. indexing policies, TTL, unique keys, creation/notification of stored procedures, triggers, UDFs are not allowed (both via SDK and Azure Portal). These have to be modified using ARM/Bicep, CLI, Powershell, REST API or Azure Management Library.
        * ARM `disableKeyBasedMetadataWriteAccess = true`
        * CLI `--disable-key-based-metadata-write-access true`
        * Powershell `-DisableKeyBasedMetadataWriteAccess true`
      * Can also be enforced via Azure Policy
    * Custom Roles
      * Need `Microsoft.DocumentDB/databaseAccounts/listKeys/*` for data access or data explorer in Azure
  * Azure Cosmos DB RBAC (Data access via RBAC)
    * Cosmos DB Built-in Data Reader (00000000-...01)
    * Cosmos DB Built-in Data Contributor (00000000-...02)
    * Scope of these assignments could be Account, Database and Container
    * Pass instance of TokenCredential class to use
    * Not supported by portal's data explorer. Must use Azure CosmosDB Explorer
    * disableLocalAuth = enabled would enforce this method only
  * Resource Tokens
    * Applicable within the database
    * Provide access to specific containers, partition keys, documents, stored procedures, triggers and UDFs
    * Created when a DB user is created and permissions are granted to that user
    * Time limited - default 1 hour, max 24 hours
    * DB can contain one or more user
    * A resource within the DB can be assigned permissions - All or Read
    * [Resource Tokens](https://learn.microsoft.com/en-us/azure/cosmos-db/secure-access-to-data?tabs=using-primary-key#resource-tokens)
  * Always encrypted using Customer Managed Keys (CMK)
    * Data is encrypted on the client side
    * id and partition key cannot be encrypted using CMK
    * Must be configured during the container creation
      * ClientEncryptionIncludedPath configuration is used
    * Encrypted properties can only be used in equality filter e.g. WHERE c.propertyName = @value
    * Performance impact
      * Point read will see 5% increase in RUs
      * Change feed will see 15% increase in RUs
      * Queries will see 15% increase in RUs
      * Write operation will see 6% increase in RUs
    * Data encryption keys (DEK)
      * Stored at database level
      * Can be shared by multiple containers
      * Can be used for one or more properties
      * Can have multiple DEKs per DB
    * Customer managed keys (CEK)
      * Used to control wrapping and unwrapping of DEK
      * Stored in Azure Key Vault (default)
    * Encryption Policy
      * Similar to indexing properties
      * Only top level properties supported
      * Encryption Type
        * Randomized - different value for encrypted data
        * Deterministic - produces same value for encrypted data (can be searched on)
      * Encryption algorithm
    * SDK
      * Initialize SDK client and attach EncryptionKeyStoreProvider (typically AzureKeyVaultKeyStoreProvider)
      * To create DEK database.CreateClientEncryptionKeyAsync..., new EncryptionKeyWrapMetadata
      * database.RewrapClientEncryptionKeyAsync..., new EncryptionKeyWrapMetadata
      * database.DefineContainer(...).WithClientEncryptionPolicy().WithIncludedPath(path1)...
      * All operations will apply the encryption/decryption as needed
      * Encrypted properties can only be used in equality filters (WHERE c.property = @Value)

* Stored Procedures
  * getContext(), getCollection(), getRequest(), getResponse()
  * Executed within context of a single container
  * Scoped to a single logical partition
  * Must execute within "server request timeout" duration
  * Create using container.Scripts.CreateStoredProcedureAsync with Microsoft.Azure.Cosmos.Scripts.StoredProcedureProperties passed in
* User Defined Functions
  * Used to extend the query language
  * Don't have context i.e. compute only
  * Create using container.Scripts.CreateUserDefinedFunctionAsync with Microsoft.Azure.Cosmos.Scripts.UserDefinedFunctionProperties passed in
* Trigger
  * getContext(), getCollection(), getRequest(), getResponse()
  * Pre-trigger don't have any input parameters
  * Post-trigger can have input parameters (optionally)
  * getRequest() to get pre-trigger object
  * getResponse() to get post-trigger object
  * Create using container.Scripts.CreateTriggerAsync with Microsoft.Azure.Cosmos.Scripts.TriggerProperties passed in
  * Triggers are not automatically executed and must be passed in as an option during SDK operations
  * These can be executed by setting x-ms-documentdb-pre-trigger-include or x-ms-documentdb-post-trigger-include headers in the POST call
* Bounded Execution
  * Stored procedures, UDFs and triggers have a timeout of 5 seconds. If the execution isn't completed within that time, the transaction is rolled back
  * CRUD operations return a boolean value that indicates if the operation would complete within the time and RU boundaries
 
    `var isAccepted = collection.createDocument(collectionLink, item, options, callback);`
* Change Feed
  * Supported for all APIs except Table and PostgreSQL
  * Items show up in the order of modification time per logical partition
  * Both pull and push models are supported. Push model is recommended
    * Push model - Azure Functions and Change feed processor
    * Pull model - Allows more control e.g. Read entire collection, range of changes and changes for specific partition key
  * Change feed processor
    * Build with container.GetChangeFeedProcessorBuilder(processorName: "changeFeedSample"...)
      * .WithInstanceName("consoleHost") should be unique for each instance
      * .WithLeaseContainer(leaseContainer) along with processorName determines if it is the same processor
      * .WithStartTime(particularPointInTime) controls when to start reading the monitored container. Using DateTime.MinValue.ToUniversalTime() would read from the beginning
    * Life-cycle notifications - WithLeaseAcquireNotification, WithLeaseReleaseNotification, WithErrorNotification
  * Pull model
    * Entity objects - `container.GetChangeFeedIterator<User>(ChangeFeedStartFrom.Beginning(), ChangeFeedMode.Incremental);`
    * Stream - `container.GetChangeFeedStreamIterator(ChangeFeedStartFrom.Beginning(), ChangeFeedMode.Incremental);`
    * Specific partition key - `container.GetChangeFeedIterator<User>(
    ChangeFeedStartFrom.Beginning(FeedRange.FromPartitionKey(new PartitionKey("PartitionKeyValue")), ChangeFeedMode.Incremental));`
    * Feed Ranges (one per physical partition) - `IReadOnlyList<FeedRange> ranges = await container.GetFeedRangesAsync();`
      * `container.GetChangeFeedIterator<User>(ChangeFeedStartFrom.Beginning(ranges[0]), ChangeFeedMode.Incremental);`
    * Consuming code is responsible for persisting the `ContinuationToken`
  * Azure Function as Change Feed Processor
    * Requires a leaseContainer
    * To share the leaseContainer across multiple functions, populate LeaseCollectionPrefix (in addition to LeaseCollectionName)
    * Populate LeaseDatabaseName if database for the lease container is not same as the database that hosts the container producing the change feed
  * Migrate from SDK v2 to v3
    * Part of v3 instead of being standalone in v2
    * Convert the DocumentCollectionInfo instances into Container references for the monitored and leases containers
    * Customizations that use WithProcessorOptions should be updated to use WithLeaseConfiguration and WithPollInterval for intervals, WithStartTime for start time, and WithMaxItems to define the maximum item count
    * Set the processorName on GetChangeFeedProcessorBuilder to match the value configured on ChangeFeedProcessorOptions.LeasePrefix, or use string.Empty otherwise
    * The changes are no longer delivered as a IReadOnlyList<Document>, instead, it's a IReadOnlyCollection<T> where T is a type you need to define, there is no base item class anymore
    * To handle the changes, you no longer need an implementation of IChangeFeedObserver, instead you need to define a delegate. The delegate can be a static Function or, if you need to maintain state across executions, you can create your own class and pass an instance method as delegate
  * Change feed estimator
    * Push - Build
      * `changeFeedEstimator = monitoredContainer
    .GetChangeFeedEstimatorBuilder("changeFeedEstimator", Program.HandleEstimationAsync, TimeSpan.FromMilliseconds(1000))`
      * Delegate in this case will receive a number that represents how many changes are pending to be read by the processor
    * On demand
      * `monitoredContainer
    .GetChangeFeedEstimator("changeFeedEstimator", leaseContainer);`
      * `estimatorIterator = changeFeedEstimator.GetCurrentStateIterator();`


* Connectors
  * Kafka connector should be configured with AvroConvertor as value.convertor which determine the serialization format
  * org.apache.kafka.connect.transforms.ReplaceField $key/$value can be used to filter or rename the fields

* Cosmos DB Analytic Store
  * Auto synced to a column store
  * Auto synced near realtime. Usually within 2 minutes but could be as high as 5 minutes for databases with shared throughput across large number of containers
  * It is schematized to optimize the performance
    * Maximum of 1000 properties across all nested levels
    * Not case sensitive
      * Property with different case in same document at same level would cause the first one to be used
      * Property with different case in different document at same level would cause both properties be represented as the first one
    * Special characters in property names are not read
    * Schema representation
      * Well-defined schema representation, default option for API for NoSQL and Gremlin accounts
        * Well-defined isn't allowed for MongoDB API
        * First document defines the base schema and properties must be of same type across for all the documents
        * Any violating properties are excluded from those documents (null is allowed)
        * Schema reset is not supported
      * Full fidelity schema representation, default option for API for MongoDB accounts
        * Can be set at account level via CLI/Powershell (not possible in Portal)
        * Achieved by representing all leaf properties as Json key-value pair including the type
        * Allows variation of data type for the same property across the documents
      * Schema type can't be changed once it is set
        * Full fidelity schema for SQL and Gremlin can only be set in CLI/Powershell
  * Analytical Time-to-Live (TTL) - time to keep the data in analytic store
    * Container level property
    * 0/NULL - Analytic store is disabled
    * -1 - Data is stored forever
    * N - data is purged after N seconds
  * Change Data Capture (CDC)
    * Doesn't affect OLTP processing or consume RUs
    * Can have multiple processes
    * Supports incremental data

* Azure Synapse Link
  * Additional column store is configured - synced near realtime
  * From Azure Synapse Analytics both transactional and analytic store is accessible
  * Synapse Apache Spark and serverless SQL pool are supported
  * Spark Pool supports read, write (through transaction store), temporary views and tables
  * Serverless SQL Pool only supports read and view
  * SQL Provisioned runtime isn't supported
  * Supported for SQL, Mongo API. In preview for Gremlin and not supported for Table and Casandara API
  * Supports periodic backup but analytic store data isn't backed up. Doesn't support continuous backup
  * Allows HTAP - Hybrid Transactional/Analytic processing
  * Once enabled, the Azure Synapse Link can't be disabled
  * Analytic Store
    * Auto sync latency is usually within 2 minutes but could be up to 5 minutes for large number of containers
  * Custom Partitioning
    * Only available for Azure Synapse Spark
    * Can have multiple partitioning strategies and multiple stores
    * Data is stored in Azure Data Lake Storage Gen2
    * Spark jobs do the incremental sync every time they run

* Azure Cognitive Search
  * Supported for SQL API. Mongo and Gremlin is in preview
  * Indexing should be consistent
  * Query must include filter as well as order by _ts column

    `
    SELECT * FROM c WHERE c.company = "microsoft" and c._ts >= @HighWaterMark ORDER BY c._ts
    `
  * DISTINCT and GROUP BY clause are not supported
  * Query can flatten the data
    
    `
    SELECT c.id, c.userId, c.contact.firstName, c.contact.lastName, c.company, c._ts FROM c WHERE c._ts >= @HighWaterMark ORDER BY c._ts
    `
    
    `
    SELECT c.id, c.userId, tag, c._ts FROM c JOIN tag IN c.tags WHERE c._ts >= @HighWaterMark ORDER BY c._ts
    `
  * Query can project the data

    `
    SELECT VALUE { "id":c.id, "Name":c.contact.firstName, "Company":c.company, "_ts":c._ts } FROM c WHERE c._ts >= @HighWaterMark ORDER BY c._ts
    `
  * Use `dataChangeDetectionPolicy` for incremental updates  
    
    `
    "dataChangeDetectionPolicy": {
          "@odata.type": "#Microsoft.Azure.Search.HighWaterMarkChangeDetectionPolicy",
          "highWaterMarkColumnName": "_ts"
    },
    `
  * If a custom query is used, the query must use `ORDER BY _ts`
  * For deletion of data to be reflected in indexer, soft delete must be used

    `
    "dataDeletionDetectionPolicy": {
        "@odata.type": "#Microsoft.Azure.Search.SoftDeleteColumnDeletionDetectionPolicy",
        "softDeleteColumnName": "isDeleted",
        "softDeleteMarkerValue": "true"
    }
    `
  * Field attributes
    * Searchable - Full-text searchable, subject to lexical analysis such as word-breaking during indexing
    * Filterable - Referenced in $filter queries, do not undergo word-breaking, so comparisons are for exact matches only
    * Sortable - By default the system sorts results by score, but fields can be configured as sortable
    * Facetable - Typically used in a presentation of search results that includes a hit count by category
    * Retrievable - Determines whether the field can be returned in a search result
    * Key - Unique identifier for documents within the index

* Kafka Connector
  * Supporter Formats
    * JSON, JSON with Schema and AVRO
  * CosmosDB can act both as source and sink
  * Sink Transforms
    * Single Message Transform (SMT) - InsertUUID can be used to insert the id field with a random UUID
    * SMT insertTTL,castTTLInt can be used to set the TTL on individual items
  * Errors
    * `
    org.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error:
    org.apache.kafka.common.errors.SerializationException: java.io.CharConversionException: Invalid UTF-32 character 0x1cfa7e2 (above 0x0010ffff) at char #1, byte #7)`
      * Data in the source topic being serialized in either Avro or another format such as CSV string
      * Use AvroConverter: `"value.converter": "io.confluent.connect.avro.AvroConverter"`
    * `
    org.apache.kafka.connect.errors.DataException: my-topic-name
    at io.confluent.connect.avro.AvroConverter.toConnectData(AvroConverter.java:97)
    org.apache.kafka.common.errors.SerializationException: Error deserializing Avro message for id -1
    org.apache.kafka.common.errors.SerializationException: Unknown magic byte!`
      * Non-Avro data is being read with AvroConverter
      * Use appropriate converter
    * `org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.`
      * JSON message without the expected schema/payload structure is being read
      * `"value.converter.schemas.enable": "false"`

* Cost Optimization
  * Point read of 1KB document will consume 1 RU
  * Point read of 100KB document will consume 10 RU
  * For strong or bounded staleness consistency levels, the RU cost of any read operation (point read or query) is doubled
  * Inserting a 1 KB item without indexing consumes ~5.5 RUs
  * Replacing an item consumes two times the RU for inserting the same item
  * Multi-region accounts cost N times of single-region accounts
    * Multi-region multi-write accounts cost 2N times of single-region accounts
  * For underutilized read regions, change feed can be used to balance
  * [Optimize request cost in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/optimize-cost-reads-writes)
  * [Optimize multi-region cost in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/optimize-cost-regions)

* Misc
  * Azure functions v1 throws "PartitionKey must be supplied with this operation". This is resolved by upgrading to v2
  * Scale up operation could complete instantly or run asynchronously which can take 4-6 hours to complete based on requested RUs and physical partition layout
    * If during the scale up operation, failover is initiated or a new region is added, the scale up operation would be paused and it would resume automatically after the failover or the reginal addition is completed
  * Scale down always complete instantly
